what worked
1. reduce host to device memcpy by keeping tree on device
2. early initialisation of cuda system via cudaFree
3. having each thread write to tree node directly
4. early one-time allocation of all buffers using precomputed size
5. using pinned memory during host array allocation

what didnt work
1.  async memcpy to host after a layer is expanded
why not: kernel execution too quick, memcpyasync would still bottleneck runtime
2. cudaMallocManaged
why not: slowed down runtime due to on-demand copying
